{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82955e7c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-18T19:00:44.761300Z",
     "iopub.status.busy": "2025-04-18T19:00:44.760996Z",
     "iopub.status.idle": "2025-04-18T19:01:00.688593Z",
     "shell.execute_reply": "2025-04-18T19:01:00.687511Z"
    },
    "papermill": {
     "duration": 15.934366,
     "end_time": "2025-04-18T19:01:00.690833",
     "exception": false,
     "start_time": "2025-04-18T19:00:44.756467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetV2S\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable mixed precision for memory efficiency\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')  # Use mixed precision\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 9\n",
    "LEARNING_RATE = 1e-4\n",
    "IMAGE_SIZE = (224, 224)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Class names (matching your folder structure)\n",
    "class_names = ['beauty', 'family', 'fashion', 'fitness', 'food', 'interior', 'other', 'pet', 'travel']\n",
    "\n",
    "# Create output directory for saved models\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Path to your preprocessed data\n",
    "data_dir = '/kaggle/input/image-classification/tfCompressedPreprocessedImages'\n",
    "\n",
    "print(\"Libraries and configuration loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432d0a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:01:00.703556Z",
     "iopub.status.busy": "2025-04-18T19:01:00.702835Z",
     "iopub.status.idle": "2025-04-18T19:01:01.812062Z",
     "shell.execute_reply": "2025-04-18T19:01:01.811094Z"
    },
    "papermill": {
     "duration": 1.116354,
     "end_time": "2025-04-18T19:01:01.813299",
     "exception": false,
     "start_time": "2025-04-18T19:01:00.696945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enhanced data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip('horizontal'),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.1),\n",
    "    tf.keras.layers.RandomContrast(0.1),\n",
    "    tf.keras.layers.RandomTranslation(height_factor=0.1, width_factor=0.1),\n",
    "])\n",
    "\n",
    "# Custom learning rate scheduler with warmup\n",
    "class WarmupCosineDecayScheduler(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, learning_rate_base, total_steps, warmup_steps, hold_base_rate_steps=0):\n",
    "        super(WarmupCosineDecayScheduler, self).__init__()\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.hold_base_rate_steps = hold_base_rate_steps\n",
    "        self.learning_rates = []\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        lr = self.get_lr(batch)\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
    "        self.learning_rates.append(lr)\n",
    "\n",
    "    def get_lr(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return self.learning_rate_base * (step / self.warmup_steps)\n",
    "        \n",
    "        if step < self.warmup_steps + self.hold_base_rate_steps:\n",
    "            return self.learning_rate_base\n",
    "        \n",
    "        step = step - self.warmup_steps - self.hold_base_rate_steps\n",
    "        total_steps = self.total_steps - self.warmup_steps - self.hold_base_rate_steps\n",
    "        \n",
    "        cosine_decay = 0.5 * (1 + np.cos(np.pi * step / total_steps))\n",
    "        return self.learning_rate_base * cosine_decay\n",
    "\n",
    "print(\"Augmentation and callbacks defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89dc316",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:01:01.820239Z",
     "iopub.status.busy": "2025-04-18T19:01:01.820024Z",
     "iopub.status.idle": "2025-04-18T19:01:01.830707Z",
     "shell.execute_reply": "2025-04-18T19:01:01.830025Z"
    },
    "papermill": {
     "duration": 0.015584,
     "end_time": "2025-04-18T19:01:01.831786",
     "exception": false,
     "start_time": "2025-04-18T19:01:01.816202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to get file count by class for display\n",
    "def get_file_counts(data_dir):\n",
    "    counts = {}\n",
    "    for class_name in class_names:\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            counts[class_name] = len([f for f in os.listdir(class_dir) if f.endswith('.npz')])\n",
    "        else:\n",
    "            counts[class_name] = 0\n",
    "    return counts\n",
    "\n",
    "# Create efficient tf.data pipeline from npz files\n",
    "def create_dataset_from_npz_files(data_dir, class_names, batch_size=32, is_training=False):\n",
    "    \"\"\"Create a tf.data.Dataset from npz files on disk\"\"\"\n",
    "    # Create lists of files and labels\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        if not os.path.exists(class_dir):\n",
    "            print(f\"Warning: Directory {class_dir} does not exist\")\n",
    "            continue\n",
    "            \n",
    "        files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if f.endswith('.npz')]\n",
    "        print(f\"Found {len(files)} files in {class_name}\")\n",
    "        file_paths.extend(files)\n",
    "        labels.extend([idx] * len(files))\n",
    "    \n",
    "    # Function to load an image from an npz file\n",
    "    def load_npz_file(file_path, label):\n",
    "        try:\n",
    "            data = np.load(file_path.numpy().decode())\n",
    "            image = data[data.files[0]]\n",
    "            # Ensure proper data type and range\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            return image, tf.one_hot(label, depth=NUM_CLASSES)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file: {e}\")\n",
    "            # Return a placeholder image in case of error\n",
    "            return tf.zeros([224, 224, 3], dtype=tf.float32), tf.one_hot(label, depth=NUM_CLASSES)\n",
    "    \n",
    "    # Create dataset from file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "    \n",
    "    # Shuffle dataset if training\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=min(len(file_paths), 10000))\n",
    "    \n",
    "    # Map the loading function\n",
    "    dataset = dataset.map(\n",
    "        lambda file, label: tf.py_function(\n",
    "            load_npz_file, [file, label], [tf.float32, tf.float32]\n",
    "        ),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Set shapes explicitly after loading\n",
    "    dataset = dataset.map(\n",
    "        lambda x, y: (tf.ensure_shape(x, [224, 224, 3]), tf.ensure_shape(y, [NUM_CLASSES])),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    \n",
    "    # Apply data augmentation if training - now with known shapes\n",
    "    if is_training:\n",
    "        # Apply individual augmentations\n",
    "        def apply_augmentation(image, label):\n",
    "            # Random horizontal flip\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            # Random brightness\n",
    "            image = tf.image.random_brightness(image, 0.1)\n",
    "            # Random contrast\n",
    "            image = tf.image.random_contrast(image, 0.9, 1.1)\n",
    "            # Random saturation\n",
    "            image = tf.image.random_saturation(image, 0.9, 1.1)\n",
    "            return image, label\n",
    "        \n",
    "        dataset = dataset.map(apply_augmentation, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Batch and prefetch\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset, len(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa7f730",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:01:01.838368Z",
     "iopub.status.busy": "2025-04-18T19:01:01.838111Z",
     "iopub.status.idle": "2025-04-18T19:01:02.780964Z",
     "shell.execute_reply": "2025-04-18T19:01:02.780027Z"
    },
    "papermill": {
     "duration": 0.94748,
     "end_time": "2025-04-18T19:01:02.782208",
     "exception": false,
     "start_time": "2025-04-18T19:01:01.834728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "file_counts = get_file_counts(data_dir)\n",
    "print(\"Dataset file counts by class:\")\n",
    "for class_name, count in file_counts.items():\n",
    "    print(f\"  {class_name}: {count} files\")\n",
    "\n",
    "total_files = sum(file_counts.values())\n",
    "print(f\"\\nTotal files: {total_files}\")\n",
    "\n",
    "# Calculate approximate sizes for train/val/test split (80/10/10)\n",
    "train_size = int(0.8 * total_files)\n",
    "val_size = int(0.1 * total_files)\n",
    "test_size = total_files - train_size - val_size\n",
    "\n",
    "# Estimate steps per epoch\n",
    "steps_per_epoch = train_size // BATCH_SIZE\n",
    "validation_steps = val_size // BATCH_SIZE\n",
    "\n",
    "print(f\"\\nEstimated steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Estimated validation steps: {validation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f7bb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:01:02.789501Z",
     "iopub.status.busy": "2025-04-18T19:01:02.789225Z",
     "iopub.status.idle": "2025-04-18T19:01:04.607505Z",
     "shell.execute_reply": "2025-04-18T19:01:04.606611Z"
    },
    "papermill": {
     "duration": 1.823445,
     "end_time": "2025-04-18T19:01:04.608903",
     "exception": false,
     "start_time": "2025-04-18T19:01:02.785458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create datasets using efficient loading\n",
    "print(\"\\nCreating datasets...\")\n",
    "\n",
    "train_dataset, train_count = create_dataset_from_npz_files(\n",
    "    data_dir, class_names, batch_size=BATCH_SIZE, is_training=True\n",
    ")\n",
    "\n",
    "val_dataset, val_count = create_dataset_from_npz_files(\n",
    "    data_dir, class_names, batch_size=BATCH_SIZE, is_training=False\n",
    ")\n",
    "\n",
    "test_dataset, test_count = create_dataset_from_npz_files(\n",
    "    data_dir, class_names, batch_size=BATCH_SIZE, is_training=False\n",
    ")\n",
    "\n",
    "# Update steps based on actual counts\n",
    "steps_per_epoch = train_count // BATCH_SIZE\n",
    "validation_steps = val_count // BATCH_SIZE\n",
    "\n",
    "print(f\"\\nActual dataset sizes:\")\n",
    "print(f\"  Training: {train_count} images\")\n",
    "print(f\"  Validation: {val_count} images\")\n",
    "print(f\"  Test: {test_count} images\")\n",
    "print(f\"  Total: {train_count + val_count + test_count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3943e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:01:04.617518Z",
     "iopub.status.busy": "2025-04-18T19:01:04.616810Z",
     "iopub.status.idle": "2025-04-18T19:01:04.623665Z",
     "shell.execute_reply": "2025-04-18T19:01:04.622954Z"
    },
    "papermill": {
     "duration": 0.011501,
     "end_time": "2025-04-18T19:01:04.624713",
     "exception": false,
     "start_time": "2025-04-18T19:01:04.613212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to build enhanced model\n",
    "def build_advanced_model():\n",
    "    \"\"\"Build an enhanced EfficientNetV2 model with better architecture\"\"\"\n",
    "    # Create base model with pretrained weights\n",
    "    base_model = EfficientNetV2S(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(224, 224, 3),\n",
    "        include_preprocessing=False\n",
    "    )\n",
    "    \n",
    "    # Freeze the base model initially\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Create model with enhanced classification head\n",
    "    inputs = tf.keras.Input(shape=(224, 224, 3))\n",
    "    \n",
    "    # Pass through base model\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Add more capacity with deeper classification head\n",
    "    x = Dense(512, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.activations.relu(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.activations.relu(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    # Final classification layer with float32 for stability\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    # Assemble the model\n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    # Compile with appropriate optimizer and loss\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "print(\"Model building function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c57dde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:01:04.631565Z",
     "iopub.status.busy": "2025-04-18T19:01:04.631339Z",
     "iopub.status.idle": "2025-04-18T19:12:29.445667Z",
     "shell.execute_reply": "2025-04-18T19:12:29.444965Z"
    },
    "papermill": {
     "duration": 684.820033,
     "end_time": "2025-04-18T19:12:29.447810",
     "exception": false,
     "start_time": "2025-04-18T19:01:04.627777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Phase 1: Train only top layers\n",
    "print(\"Phase 1: Training only top layers...\")\n",
    "model, base_model = build_advanced_model()\n",
    "model.summary()\n",
    "\n",
    "# Setup callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=7,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'models/efficient_net_phase1.weights.h5',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Log directory for TensorBoard\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_phase1\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Train with frozen base model\n",
    "history_phase1 = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,  # Adjust as needed\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Plot phase 1 results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_phase1.history['accuracy'])\n",
    "plt.plot(history_phase1.history['val_accuracy'])\n",
    "plt.title('Phase 1 - Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_phase1.history['loss'])\n",
    "plt.plot(history_phase1.history['val_loss'])\n",
    "plt.title('Phase 1 - Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a430343",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:12:29.595715Z",
     "iopub.status.busy": "2025-04-18T19:12:29.595428Z",
     "iopub.status.idle": "2025-04-18T19:42:44.902107Z",
     "shell.execute_reply": "2025-04-18T19:42:44.901521Z"
    },
    "papermill": {
     "duration": 1815.382871,
     "end_time": "2025-04-18T19:42:44.904558",
     "exception": false,
     "start_time": "2025-04-18T19:12:29.521687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Phase 2: Unfreeze and fine-tune the top 50 layers\n",
    "print(\"Phase 2: Fine-tuning top 50 layers...\")\n",
    "\n",
    "# Unfreeze top 50 layers\n",
    "for layer in base_model.layers[-50:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=5e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Update callbacks for phase 2\n",
    "checkpoint.filepath = 'models/efficient_net_phase2.weights.h5'\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_phase2\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Train with partially unfrozen model\n",
    "history_phase2 = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,  # Adjust as needed\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard_callback]\n",
    ")\n",
    "\n",
    "# Plot phase 2 results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_phase2.history['accuracy'])\n",
    "plt.plot(history_phase2.history['val_accuracy'])\n",
    "plt.title('Phase 2 - Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_phase2.history['loss'])\n",
    "plt.plot(history_phase2.history['val_loss'])\n",
    "plt.title('Phase 2 - Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df8672",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:42:45.535292Z",
     "iopub.status.busy": "2025-04-18T19:42:45.534966Z",
     "iopub.status.idle": "2025-04-18T19:54:59.282178Z",
     "shell.execute_reply": "2025-04-18T19:54:59.281561Z"
    },
    "papermill": {
     "duration": 734.097423,
     "end_time": "2025-04-18T19:54:59.283645",
     "exception": false,
     "start_time": "2025-04-18T19:42:45.186222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Phase 3: Unfreeze and fine-tune more layers\n",
    "print(\"Phase 3: Fine-tuning more layers...\")\n",
    "\n",
    "# Unfreeze more layers\n",
    "for layer in base_model.layers[-100:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with even lower learning rate\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Update callbacks for phase 3\n",
    "checkpoint.filepath = 'models/efficient_net_phase3.weights.h5'\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"_phase3\"\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# Instead of using the WarmupCosineDecayScheduler, use a simpler approach\n",
    "# Create a learning rate schedule function\n",
    "def cosine_decay_with_warmup(epoch, lr):\n",
    "    # Total epochs and warmup epochs\n",
    "    total_epochs = epochs_phase3\n",
    "    warmup_epochs = int(0.1 * total_epochs)  # 10% of epochs for warmup\n",
    "    \n",
    "    # Warmup phase\n",
    "    if epoch < warmup_epochs:\n",
    "        return 1e-5 * ((epoch + 1) / warmup_epochs)\n",
    "    \n",
    "    # Cosine decay phase\n",
    "    decay_epochs = total_epochs - warmup_epochs\n",
    "    epoch_in_decay = epoch - warmup_epochs\n",
    "    cosine = 0.5 * (1 + np.cos(np.pi * epoch_in_decay / decay_epochs))\n",
    "    return 1e-5 * cosine\n",
    "\n",
    "# Create a learning rate scheduler callback\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(cosine_decay_with_warmup, verbose=1)\n",
    "\n",
    "# Train with more unfrozen layers - removed the warmup_lr callback\n",
    "epochs_phase3 = 10  # Adjust as needed\n",
    "history_phase3 = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs_phase3,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint, tensorboard_callback, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Plot phase 3 results\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_phase3.history['accuracy'])\n",
    "plt.plot(history_phase3.history['val_accuracy'])\n",
    "plt.title('Phase 3 - Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_phase3.history['loss'])\n",
    "plt.plot(history_phase3.history['val_loss'])\n",
    "plt.title('Phase 3 - Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save final model\n",
    "model.save('models/efficient_net_final.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab0db1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:54:59.994808Z",
     "iopub.status.busy": "2025-04-18T19:54:59.994507Z",
     "iopub.status.idle": "2025-04-18T19:55:00.593664Z",
     "shell.execute_reply": "2025-04-18T19:55:00.593059Z"
    },
    "papermill": {
     "duration": 0.954447,
     "end_time": "2025-04-18T19:55:00.596195",
     "exception": false,
     "start_time": "2025-04-18T19:54:59.641748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine histories\n",
    "combined_history = {\n",
    "    'accuracy': (\n",
    "        history_phase1.history['accuracy'] + \n",
    "        history_phase2.history['accuracy'] + \n",
    "        history_phase3.history['accuracy']\n",
    "    ),\n",
    "    'val_accuracy': (\n",
    "        history_phase1.history['val_accuracy'] + \n",
    "        history_phase2.history['val_accuracy'] + \n",
    "        history_phase3.history['val_accuracy']\n",
    "    ),\n",
    "    'loss': (\n",
    "        history_phase1.history['loss'] + \n",
    "        history_phase2.history['loss'] + \n",
    "        history_phase3.history['loss']\n",
    "    ),\n",
    "    'val_loss': (\n",
    "        history_phase1.history['val_loss'] + \n",
    "        history_phase2.history['val_loss'] + \n",
    "        history_phase3.history['val_loss']\n",
    "    )\n",
    "}\n",
    "\n",
    "# Plot overall training history\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(combined_history['accuracy'])\n",
    "plt.plot(combined_history['val_accuracy'])\n",
    "plt.title('Overall Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.axvline(x=len(history_phase1.history['accuracy']), color='r', linestyle='--')\n",
    "plt.axvline(x=len(history_phase1.history['accuracy']) + len(history_phase2.history['accuracy']), \n",
    "            color='r', linestyle='--')\n",
    "plt.legend(['Train', 'Validation', 'Phase Change'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(combined_history['loss'])\n",
    "plt.plot(combined_history['val_loss'])\n",
    "plt.title('Overall Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.axvline(x=len(history_phase1.history['loss']), color='r', linestyle='--')\n",
    "plt.axvline(x=len(history_phase1.history['loss']) + len(history_phase2.history['loss']), \n",
    "            color='r', linestyle='--')\n",
    "plt.legend(['Train', 'Validation', 'Phase Change'])\n",
    "plt.tight_layout()\n",
    "plt.savefig('overall_training_history.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc89f88e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:55:01.425522Z",
     "iopub.status.busy": "2025-04-18T19:55:01.425204Z",
     "iopub.status.idle": "2025-04-18T19:57:14.833541Z",
     "shell.execute_reply": "2025-04-18T19:57:14.832763Z"
    },
    "papermill": {
     "duration": 133.7836,
     "end_time": "2025-04-18T19:57:14.835066",
     "exception": false,
     "start_time": "2025-04-18T19:55:01.051466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to evaluate model on test set\n",
    "def evaluate_model(model, test_dataset):\n",
    "    \"\"\"Evaluate model on test set and visualize results\"\"\"\n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    \n",
    "    for images, labels in test_dataset:\n",
    "        batch_predictions = model.predict(images)\n",
    "        y_pred.extend(np.argmax(batch_predictions, axis=1))\n",
    "        y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
    "    \n",
    "    # Convert to arrays\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return test_accuracy, y_true, y_pred\n",
    "\n",
    "# Evaluate the trained model\n",
    "test_accuracy, y_true, y_pred = evaluate_model(model, test_dataset)\n",
    "print(f\"Final test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6867b0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:57:15.697416Z",
     "iopub.status.busy": "2025-04-18T19:57:15.696404Z",
     "iopub.status.idle": "2025-04-18T19:57:16.824035Z",
     "shell.execute_reply": "2025-04-18T19:57:16.823264Z"
    },
    "papermill": {
     "duration": 1.52216,
     "end_time": "2025-04-18T19:57:16.825232",
     "exception": false,
     "start_time": "2025-04-18T19:57:15.303072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a feature extractor for multimodal model\n",
    "def create_feature_extractor(model):\n",
    "    \"\"\"Create and save a feature extractor for multimodal integration\"\"\"\n",
    "    # Create a new model that outputs features\n",
    "    feature_extractor = tf.keras.models.Model(\n",
    "        inputs=model.input,\n",
    "        outputs=model.get_layer('global_average_pooling2d').output\n",
    "    )\n",
    "    \n",
    "    feature_extractor.save('models/image_feature_extractor.keras')\n",
    "    print(\"Feature extractor saved for multimodal model integration!\")\n",
    "    \n",
    "    return feature_extractor\n",
    "\n",
    "# Create and save the feature extractor\n",
    "feature_extractor = create_feature_extractor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b348bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T19:57:17.615206Z",
     "iopub.status.busy": "2025-04-18T19:57:17.614926Z",
     "iopub.status.idle": "2025-04-18T19:57:26.927919Z",
     "shell.execute_reply": "2025-04-18T19:57:26.927128Z"
    },
    "papermill": {
     "duration": 9.726942,
     "end_time": "2025-04-18T19:57:26.948988",
     "exception": false,
     "start_time": "2025-04-18T19:57:17.222046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "def visualize_predictions(model, test_dataset, class_names, num_samples=10):\n",
    "    \"\"\"Visualize model predictions on test images\"\"\"\n",
    "    # Get a batch of test images\n",
    "    for images, labels in test_dataset:\n",
    "        # Only take the first num_samples\n",
    "        if images.shape[0] >= num_samples:\n",
    "            batch_images = images[:num_samples]\n",
    "            batch_labels = labels[:num_samples]\n",
    "            break\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(batch_images)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        \n",
    "        # Convert from [-1,1] to [0,1] range for display\n",
    "        img = (batch_images[i] + 1) / 2.0\n",
    "        plt.imshow(img)\n",
    "        \n",
    "        true_label = np.argmax(batch_labels[i])\n",
    "        pred_label = np.argmax(predictions[i])\n",
    "        \n",
    "        title_color = 'green' if true_label == pred_label else 'red'\n",
    "        plt.title(f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\", \n",
    "                  color=title_color)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_samples.png')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some test predictions\n",
    "visualize_predictions(model, test_dataset, class_names)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7019195,
     "sourceId": 11235922,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3410.416614,
   "end_time": "2025-04-18T19:57:30.979691",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-18T19:00:40.563077",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
